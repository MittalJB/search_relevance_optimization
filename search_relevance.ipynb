{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c5ad81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in .\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in .\\.venv\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in .\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in .\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in .\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>name</th>\n",
       "      <th>sub_title</th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>color</th>\n",
       "      <th>price</th>\n",
       "      <th>currency</th>\n",
       "      <th>availability</th>\n",
       "      <th>description</th>\n",
       "      <th>raw_description</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>review_count</th>\n",
       "      <th>images</th>\n",
       "      <th>available_sizes</th>\n",
       "      <th>uniq_id</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>Title</th>\n",
       "      <th>Subtitle</th>\n",
       "      <th>Product Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.nike.com/t/dri-fit-team-minnesota-...</td>\n",
       "      <td>Nike Dri-FIT Team (MLB Minnesota Twins)</td>\n",
       "      <td>Men's Long-Sleeve T-Shirt</td>\n",
       "      <td>Nike</td>\n",
       "      <td>14226571.0</td>\n",
       "      <td>Navy</td>\n",
       "      <td>40.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>InStock</td>\n",
       "      <td>SWEAT-WICKING COMFORT.The Nike Dri-FIT Team (M...</td>\n",
       "      <td>&lt;div class=\"pi-pdpmainbody\"&gt;&lt;p&gt;&lt;b class=\"headl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://static.nike.com/a/images/t_PDP_1280_v1...</td>\n",
       "      <td>S | M | L | XL | 2XL</td>\n",
       "      <td>c3229e54-aa58-5fdd-9f71-fbe66366b2b2</td>\n",
       "      <td>20/09/2022 23:32:28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.nike.com/t/club-américa-womens-dri...</td>\n",
       "      <td>Club América</td>\n",
       "      <td>Women's Nike Dri-FIT Soccer Jersey Dress</td>\n",
       "      <td>Nike</td>\n",
       "      <td>13814665.0</td>\n",
       "      <td>Black/Black</td>\n",
       "      <td>90.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>InStock</td>\n",
       "      <td>Inspired by traditional soccer jerseys, the Cl...</td>\n",
       "      <td>&lt;div class=\"pi-pdpmainbody\"&gt;&lt;br/&gt;&lt;p&gt;Inspired b...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://static.nike.com/a/images/t_PDP_1280_v1...</td>\n",
       "      <td>L (12–14)</td>\n",
       "      <td>f8ebb2ed-17ae-5719-b750-5ea3ec69b75c</td>\n",
       "      <td>20/09/2022 23:32:40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.nike.com/t/sportswear-swoosh-mens-...</td>\n",
       "      <td>Nike Sportswear Swoosh</td>\n",
       "      <td>Men's Overalls</td>\n",
       "      <td>Nike</td>\n",
       "      <td>13015648.0</td>\n",
       "      <td>Black/White</td>\n",
       "      <td>140.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>OutOfStock</td>\n",
       "      <td>WORKING HARD TO KEEP YOU COMFORTABLE.The Nike ...</td>\n",
       "      <td>&lt;div class=\"pi-pdpmainbody\"&gt;&lt;p&gt;&lt;b class=\"headl...</td>\n",
       "      <td>4.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>https://static.nike.com/a/images/t_PDP_1280_v1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88120081-e6cb-5399-b9dc-a2d3d5dd5206</td>\n",
       "      <td>20/09/2022 23:33:16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.nike.com/t/dri-fit-one-luxe-big-ki...</td>\n",
       "      <td>Nike Dri-FIT One Luxe</td>\n",
       "      <td>Big Kids' (Girls') Printed Tights (Extended Size)</td>\n",
       "      <td>Nike</td>\n",
       "      <td>13809796.0</td>\n",
       "      <td>Black/Rush Pink</td>\n",
       "      <td>22.97</td>\n",
       "      <td>USD</td>\n",
       "      <td>OutOfStock</td>\n",
       "      <td>ELEVATED COMFORT GOES FULL BLOOM.The Nike Dri-...</td>\n",
       "      <td>&lt;div class=\"pi-pdpmainbody\"&gt;&lt;p&gt;&lt;b class=\"headl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://static.nike.com/a/images/t_PDP_1280_v1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98348cc5-1520-5b6e-a5f6-c42547b6a092</td>\n",
       "      <td>20/09/2022 23:33:17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.nike.com/t/paris-saint-germain-rep...</td>\n",
       "      <td>Paris Saint-Germain Repel Academy AWF</td>\n",
       "      <td>Big Kids' Soccer Jacket</td>\n",
       "      <td>Nike</td>\n",
       "      <td>13327415.0</td>\n",
       "      <td>Dark Grey/Black/Siren Red/Siren Red</td>\n",
       "      <td>70.00</td>\n",
       "      <td>USD</td>\n",
       "      <td>InStock</td>\n",
       "      <td>WATER-REPELLENT COVERAGE GETS PSG DETAILS.The ...</td>\n",
       "      <td>&lt;div class=\"pi-pdpmainbody\"&gt;&lt;p&gt;&lt;b class=\"headl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://static.nike.com/a/images/t_PDP_1280_v1...</td>\n",
       "      <td>XS | S | M | L | XL</td>\n",
       "      <td>f15981a5-d8c9-53fa-880d-80606be188fe</td>\n",
       "      <td>20/09/2022 23:33:22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.nike.com/t/dri-fit-team-minnesota-...   \n",
       "1  https://www.nike.com/t/club-américa-womens-dri...   \n",
       "2  https://www.nike.com/t/sportswear-swoosh-mens-...   \n",
       "3  https://www.nike.com/t/dri-fit-one-luxe-big-ki...   \n",
       "4  https://www.nike.com/t/paris-saint-germain-rep...   \n",
       "\n",
       "                                      name  \\\n",
       "0  Nike Dri-FIT Team (MLB Minnesota Twins)   \n",
       "1                             Club América   \n",
       "2                   Nike Sportswear Swoosh   \n",
       "3                    Nike Dri-FIT One Luxe   \n",
       "4    Paris Saint-Germain Repel Academy AWF   \n",
       "\n",
       "                                           sub_title brand       model  \\\n",
       "0                          Men's Long-Sleeve T-Shirt  Nike  14226571.0   \n",
       "1           Women's Nike Dri-FIT Soccer Jersey Dress  Nike  13814665.0   \n",
       "2                                     Men's Overalls  Nike  13015648.0   \n",
       "3  Big Kids' (Girls') Printed Tights (Extended Size)  Nike  13809796.0   \n",
       "4                            Big Kids' Soccer Jacket  Nike  13327415.0   \n",
       "\n",
       "                                 color   price currency availability  \\\n",
       "0                                 Navy   40.00      USD      InStock   \n",
       "1                          Black/Black   90.00      USD      InStock   \n",
       "2                          Black/White  140.00      USD   OutOfStock   \n",
       "3                      Black/Rush Pink   22.97      USD   OutOfStock   \n",
       "4  Dark Grey/Black/Siren Red/Siren Red   70.00      USD      InStock   \n",
       "\n",
       "                                         description  \\\n",
       "0  SWEAT-WICKING COMFORT.The Nike Dri-FIT Team (M...   \n",
       "1  Inspired by traditional soccer jerseys, the Cl...   \n",
       "2  WORKING HARD TO KEEP YOU COMFORTABLE.The Nike ...   \n",
       "3  ELEVATED COMFORT GOES FULL BLOOM.The Nike Dri-...   \n",
       "4  WATER-REPELLENT COVERAGE GETS PSG DETAILS.The ...   \n",
       "\n",
       "                                     raw_description  avg_rating  \\\n",
       "0  <div class=\"pi-pdpmainbody\"><p><b class=\"headl...         NaN   \n",
       "1  <div class=\"pi-pdpmainbody\"><br/><p>Inspired b...         5.0   \n",
       "2  <div class=\"pi-pdpmainbody\"><p><b class=\"headl...         4.9   \n",
       "3  <div class=\"pi-pdpmainbody\"><p><b class=\"headl...         NaN   \n",
       "4  <div class=\"pi-pdpmainbody\"><p><b class=\"headl...         NaN   \n",
       "\n",
       "   review_count                                             images  \\\n",
       "0           NaN  https://static.nike.com/a/images/t_PDP_1280_v1...   \n",
       "1           1.0  https://static.nike.com/a/images/t_PDP_1280_v1...   \n",
       "2          11.0  https://static.nike.com/a/images/t_PDP_1280_v1...   \n",
       "3           NaN  https://static.nike.com/a/images/t_PDP_1280_v1...   \n",
       "4           NaN  https://static.nike.com/a/images/t_PDP_1280_v1...   \n",
       "\n",
       "        available_sizes                               uniq_id  \\\n",
       "0  S | M | L | XL | 2XL  c3229e54-aa58-5fdd-9f71-fbe66366b2b2   \n",
       "1             L (12–14)  f8ebb2ed-17ae-5719-b750-5ea3ec69b75c   \n",
       "2                   NaN  88120081-e6cb-5399-b9dc-a2d3d5dd5206   \n",
       "3                   NaN  98348cc5-1520-5b6e-a5f6-c42547b6a092   \n",
       "4   XS | S | M | L | XL  f15981a5-d8c9-53fa-880d-80606be188fe   \n",
       "\n",
       "            scraped_at Title Subtitle Product Description  \n",
       "0  20/09/2022 23:32:28   NaN      NaN                 NaN  \n",
       "1  20/09/2022 23:32:40   NaN      NaN                 NaN  \n",
       "2  20/09/2022 23:33:16   NaN      NaN                 NaN  \n",
       "3  20/09/2022 23:33:17   NaN      NaN                 NaN  \n",
       "4  20/09/2022 23:33:22   NaN      NaN                 NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install pandas\n",
    "import pandas as pd \n",
    "df1 = pd.read_csv(\"nike-data-1.csv\")\n",
    "df2 = pd. read_csv(\"nike-data-2.csv\")\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "combined_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d371f683",
   "metadata": {},
   "source": [
    "Data Preprocessing (with NLTK + Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e25ec33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in .\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in .\\.venv\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in .\\.venv\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in .\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in .\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in .\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     d:\\search_relevance_project\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     d:\\search_relevance_project\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Mittal/nltk_data'\n    - 'd:\\\\search_relevance_project\\\\.venv\\\\nltk_data'\n    - 'd:\\\\search_relevance_project\\\\.venv\\\\share\\\\nltk_data'\n    - 'd:\\\\search_relevance_project\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Mittal\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'd:\\\\search_relevance_project\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join([w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m w.isalnum() \u001b[38;5;129;01mand\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words])\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Apply to your dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m combined_df[\u001b[33m'\u001b[39m\u001b[33mclean_description\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mcombined_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdescription\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\search_relevance_project\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\search_relevance_project\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\search_relevance_project\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\search_relevance_project\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\search_relevance_project\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mclean_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pd.isnull(text): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     24\u001b[39m text = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m<[^>]+>\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m tokens = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m stop_words = \u001b[38;5;28mset\u001b[39m(stopwords.words(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join([w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m w.isalnum() \u001b[38;5;129;01mand\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\search_relevance_project\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\search_relevance_project\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\search_relevance_project\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\search_relevance_project\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\search_relevance_project\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\search_relevance_project\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Mittal/nltk_data'\n    - 'd:\\\\search_relevance_project\\\\.venv\\\\nltk_data'\n    - 'd:\\\\search_relevance_project\\\\.venv\\\\share\\\\nltk_data'\n    - 'd:\\\\search_relevance_project\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Mittal\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'd:\\\\search_relevance_project\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# Create and set NLTK download directory explicitly\n",
    "nltk_data_path = os.path.join(os.getcwd(), \"nltk_data\")\n",
    "os.makedirs(nltk_data_path, exist_ok=True)\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "nltk.download('stopwords', download_dir=nltk_data_path)\n",
    "\n",
    "# Tell NLTK to look there\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Now re-import and use\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text): return \"\"\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join([w for w in tokens if w.isalnum() and w not in stop_words])\n",
    "\n",
    "# Apply to your dataset\n",
    "\n",
    "combined_df['clean_description'] = combined_df['description'].apply(clean_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
